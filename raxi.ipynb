{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theoneandtheonlytaghyan/DEPI-Assignment/blob/main/raxi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66IrlOLRxH2y",
        "outputId": "06d205a4-639d-4c71-a5ad-a3497776dd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.6/363.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas sentence-transformers scikit-learn torch nltk spacy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KGIGsocxKBh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjvo8c4UyrfT"
      },
      "outputs": [],
      "source": [
        "def clean_composition(text: str):\n",
        "    \"\"\"\n",
        "    تُنظف دالة clean_composition عمود \"Composition\" لاستخلاص:\n",
        "      - اسم المادة الفعالة (قبل الأقواس)\n",
        "      - التركيز الموجود داخل الأقواس\n",
        "    مثال:\n",
        "      \"ambroxol (30mg/5ml)\" تُرجع (\"ambroxol\", \"30mg/5ml\")\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\", None\n",
        "    # الحصول على الجزء الأول قبل علامة الجمع (إن وُجد)\n",
        "    main_part = text.split(\" + \")[0]\n",
        "    # استخراج المحتوى داخل الأقواس\n",
        "    match = re.search(r'\\(([^)]+)\\)', main_part)\n",
        "    concentration = match.group(1) if match else None\n",
        "    # إزالة المحتوى بين الأقواس للحصول على الاسم النقي\n",
        "    composition_clean = re.sub(r'\\s*\\([^)]+\\)', '', main_part)\n",
        "    return composition_clean.strip(), concentration\n",
        "\n",
        "# اختبار الدالة\n",
        "example_text = \"Ambroxol (30mg/5ml)\"\n",
        "print(\"Test clean_composition:\", clean_composition(example_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZx3OPpAx42B"
      },
      "outputs": [],
      "source": [
        "# تحميل البيانات\n",
        "df = pd.read_csv('/content/raxi.csv')\n",
        "\n",
        "# عرض البيانات\n",
        "# تطبيق تنظيف عمود Composition لإنشاء الأعمدة composition_clean و concentration\n",
        "df[['composition_clean', 'concentration']] = df['Composition'].apply(lambda x: pd.Series(clean_composition(x)))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av7wJbhuyEbz"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rashcn6xz1dQ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmltIJdj0AFU"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTUJQcOc0C23"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWlOytTP0HNw"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obSzgI7x0KO1"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-3d5lHm0VC8"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_R8DZkK0Wtp"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNmeYdhq0bh0"
      },
      "outputs": [],
      "source": [
        "df = df.drop('Composition', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDYp86vq02-4"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLOd3j1D05SA"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDAA97Nz090Q"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())\n",
        "print(df.describe())\n",
        "print(df.dtypes)\n",
        "print(df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZLwOylt1O_s"
      },
      "outputs": [],
      "source": [
        "print(df['concentration'].value_counts())\n",
        "print(df['composition_clean'].value_counts())\n",
        "print(df['Uses'].value_counts())\n",
        "print(df['Side_effects'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1FKAvVn1zm1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDuQ2BZj26rt"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq9Ff6JQ29Tw"
      },
      "outputs": [],
      "source": [
        "df['text'] = df['Uses'] + ' ' + df['Side_effects']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8U5eqZZ6Rj1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# Ensure the 'text' column contains only strings\n",
        "df['text'] = df['text'].astype(str)\n",
        "df['tokens'] = df['text'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEnJ4Ctb3UbG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') # Downloading stopwords dataset\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo4m_pv-3enp"
      },
      "outputs": [],
      "source": [
        "\n",
        "nltk.download('wordnet') # Downloading wordnet dataset\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxmxFy-e3Z18"
      },
      "outputs": [],
      "source": [
        "df['text'] = df['tokens'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkX_kjFS3mUi"
      },
      "outputs": [],
      "source": [
        "encodings = tokenizer(list(df['text']), truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8rB1vZr3pY4"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# تحميل النموذج\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# تحويل النصوص إلى تمثيلات رياضية\n",
        "texts = [str(row['Uses']) + ' ' + str(row['Side_effects']) for index, row in df.iterrows()]\n",
        "embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# تحليل النتائج\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SBlJJmC4cSz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# حساب تشابه بين النصوص\n",
        "similarities = cosine_similarity(embeddings)\n",
        "print(similarities.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6o8WCRZzC6Xt"
      },
      "outputs": [],
      "source": [
        "# تحديد الأدوية التي لها تأثيرات جانبية مشابهة\n",
        "similar_drugs = []\n",
        "for i in range(len(similarities)):\n",
        "    for j in range(i+1, len(similarities)):\n",
        "        if similarities[i][j] > 0.8:  # حد التشابه\n",
        "            similar_drugs.append((df.iloc[i]['Medicine Name'], df.iloc[j]['Medicine Name']))\n",
        "\n",
        "print(similar_drugs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlN-ZG1IC9Q3"
      },
      "outputs": [],
      "source": [
        "# تحليل النتائج\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# رسم توزيع التشابهات\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(similarities.flatten(), bins=50)\n",
        "plt.title('Distribution of Similarities')\n",
        "plt.xlabel('Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAwWDlPbIrLB"
      },
      "outputs": [],
      "source": [
        "def calculate_compatibility(prescription, dispensed_medicine, df, model):\n",
        "    \"\"\"\n",
        "    تحسب درجة التوافق بين الروشتة والدواء المصروف.\n",
        "\n",
        "    Args:\n",
        "        prescription (dict): بيانات الروشتة.\n",
        "        dispensed_medicine (str): اسم الدواء المصروف.\n",
        "        df (DataFrame): البيانات.\n",
        "        model (SentenceTransformer): النموذج.\n",
        "\n",
        "    Returns:\n",
        "        float: درجة التوافق بنسبة مئوية.\n",
        "    \"\"\"\n",
        "\n",
        "    # البحث عن الدواء المصروف في البيانات\n",
        "    medicine_data = df[df['Medicine Name'] == dispensed_medicine]\n",
        "    # Check if medicine_data is empty before accessing iloc[0]\n",
        "    if medicine_data.empty:\n",
        "        return 0  # إذا لم يتم العثور على الدواء، نعيد 0%\n",
        "    else:\n",
        "        medicine_data = medicine_data.iloc[0]\n",
        "\n",
        "    # تحويل النصوص إلى تمثيلات رياضية\n",
        "    prescription_uses_embedding = model.encode(prescription['Uses'])\n",
        "    dataset_uses_embedding = model.encode(medicine_data['Uses'])\n",
        "    prescription_side_effects_embedding = model.encode(prescription['Side_effects'])\n",
        "    dataset_side_effects_embedding = model.encode(medicine_data['Side_effects'])\n",
        "\n",
        "    # حساب التشابه بين النصوص\n",
        "    uses_similarity = util.cos_sim(prescription_uses_embedding, dataset_uses_embedding).item()\n",
        "    side_effects_similarity = util.cos_sim(prescription_side_effects_embedding, dataset_side_effects_embedding).item()\n",
        "\n",
        "    # مقارنة المادة الفعالة والتركيز\n",
        "    composition_match = prescription['composition_clean'] == medicine_data['composition_clean']\n",
        "    concentration_match = prescription['concentration'] == medicine_data['concentration']\n",
        "\n",
        "    # حساب درجة التوافق\n",
        "    composition_score = 1 if composition_match else 0\n",
        "    concentration_score = 1 if concentration_match else 0\n",
        "\n",
        "    # حساب درجة التوافق النهائية\n",
        "    compatibility_score = (0.4 * composition_score + 0.4 * concentration_score + 0.1 * uses_similarity + 0.1 * side_effects_similarity) * 100\n",
        "\n",
        "    return compatibility_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUitG64BHwT0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# إنشاء مجموعة بيانات اختبار\n",
        "test_prescriptions = [\n",
        "    {'composition_clean': 'Paracetamol', 'concentration': '500mg', 'Uses': 'Headache, Fever', 'Side_effects': 'Nausea, dizziness'},\n",
        "    {'composition_clean': 'Ibuprofen', 'concentration': '200mg', 'Uses': 'Pain, Inflammation', 'Side_effects': 'Stomach upset, Headache'},\n",
        "\n",
        "]\n",
        "\n",
        "test_medicines = ['Panadol Extra', 'Advil', 'Tylenol', 'Motrin']  # إضافة الأدوية المصروفة المناسبة\n",
        "\n",
        "# حساب درجات التوافق للبيانات الاختبارية\n",
        "test_scores = []\n",
        "for prescription in test_prescriptions:\n",
        "    for medicine in test_medicines:\n",
        "        score = calculate_compatibility(prescription, medicine, df, model)\n",
        "        test_scores.append(score)\n",
        "\n",
        "# تقييم أداء النموذج\n",
        "actual_scores = [90, 80, 10, 5, 95, 85, 15, 10]  # إضافة الدرجات الفعلية للتوافق\n",
        "mse = mean_squared_error(actual_scores, test_scores)\n",
        "print(f'Mean Squared Error: {mse:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ2pum-eJh9I"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import Dataset, DatasetDict # Import Dataset and DatasetDict\n",
        "from sentence_transformers import InputExample, losses, SentenceTransformer, util\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# إعداد بيانات التدريب\n",
        "train_examples = []\n",
        "for _, row in df.iterrows():\n",
        "    # Ensure 'Uses' and 'Side_effects' are strings\n",
        "    uses = str(row['Uses'])\n",
        "    side_effects = str(row['Side_effects'])\n",
        "    train_examples.append(InputExample(texts=[uses, side_effects], label=1.0))\n",
        "\n",
        "# Convert train_examples to a Hugging Face Dataset\n",
        "train_dataset = Dataset.from_list([{\"texts\": example.texts, \"label\": example.label} for example in train_examples])\n",
        "# Wrap the dataset in a DatasetDict\n",
        "train_dataset = DatasetDict({\"train\": train_dataset})\n",
        "\n",
        "# Create a dataloader from the Hugging Face Dataset\n",
        "train_dataloader = DataLoader(train_dataset['train'], shuffle=True, batch_size=16)\n",
        "# حساب درجات التوافق بعد التدريب\n",
        "fine_tuned_scores = []\n",
        "for prescription in test_prescriptions:\n",
        "    for medicine in test_medicines:\n",
        "        score = calculate_compatibility(prescription, medicine, df, model)\n",
        "        fine_tuned_scores.append(score)\n",
        "\n",
        "# تقييم أداء النموذج بعد التدريب\n",
        "fine_tuned_mse = mean_squared_error(actual_scores, fine_tuned_scores)\n",
        "print(f'Fine-tuned Mean Squared Error: {fine_tuned_mse:.2f}')\n",
        "\n",
        "# رسم توزيع درجات التوافق\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(test_scores, bins=20, alpha=0.5, label='Original')\n",
        "plt.hist(fine_tuned_scores, bins=20, alpha=0.5, label='Fine-tuned')\n",
        "plt.title('Distribution of Compatibility Scores')\n",
        "plt.xlabel('Compatibility Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guJysmkuKZOi"
      },
      "outputs": [],
      "source": [
        "# مثال على استخدام النموذج لتحديد ملائمتها للاستخدام\n",
        "new_prescription = {\n",
        "    'composition_clean': 'Bevacizumab',\n",
        "    'concentration': '400mg',\n",
        "    'Uses': ' Cancer of colon ',\n",
        "    'Side_effects': 'Headache '\n",
        "}\n",
        "\n",
        "new_medicine = 'Avastin 400mg Injection'\n",
        "\n",
        "new_compatibility_score = calculate_compatibility(new_prescription, new_medicine, df, model)\n",
        "print(f'New Compatibility Score: {new_compatibility_score:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6ZXkkEffNtk"
      },
      "outputs": [],
      "source": [
        "# دالة لحساب درجة التوافق\n",
        "def calculate_compatibility(prescription, dispensed_medicine, df, model):\n",
        "    \"\"\"\n",
        "    تحسب درجة التوافق بين الروشتة والدواء المصروف.\n",
        "\n",
        "    Args:\n",
        "        prescription (dict): بيانات الروشتة.\n",
        "        dispensed_medicine (str): اسم الدواء المصروف.\n",
        "        df (DataFrame): البيانات.\n",
        "        model (SentenceTransformer): النموذج.\n",
        "\n",
        "    Returns:\n",
        "        float: درجة التوافق بنسبة مئوية.\n",
        "    \"\"\"\n",
        "\n",
        "    # البحث عن الدواء المصروف في البيانات\n",
        "    medicine_data = df[df['Medicine Name'] == dispensed_medicine]\n",
        "    if medicine_data.empty:\n",
        "        print(f\"Error: Medicine '{dispensed_medicine}' not found in the database.\")\n",
        "        return 0  # Or raise an exception, etc.\n",
        "    else:\n",
        "        medicine_data = medicine_data.iloc[0]\n",
        "\n",
        "    # تحويل النصوص إلى تمثيلات رياضية\n",
        "    prescription_uses_embedding = model.encode(prescription['Uses'])\n",
        "    dataset_uses_embedding = model.encode(medicine_data['Uses'])\n",
        "    prescription_side_effects_embedding = model.encode(prescription['Side_effects'])\n",
        "    dataset_side_effects_embedding = model.encode(medicine_data['Side_effects'])\n",
        "\n",
        "    # حساب التشابه بين النصوص\n",
        "    uses_similarity = util.cos_sim(prescription_uses_embedding, dataset_uses_embedding).item()\n",
        "    side_effects_similarity = util.cos_sim(prescription_side_effects_embedding, dataset_side_effects_embedding).item()\n",
        "\n",
        "    # مقارنة المادة الفعالة والتركيز\n",
        "    composition_match = prescription['composition_clean'] == medicine_data['composition_clean']\n",
        "    concentration_match = prescription['concentration'] == medicine_data['concentration']\n",
        "\n",
        "    # حساب درجة التوافق\n",
        "    composition_score = 1 if composition_match else 0\n",
        "    concentration_score = 1 if concentration_match else 0\n",
        "\n",
        "    # حساب درجة التوافق النهائية\n",
        "    compatibility_score = (0.4 * composition_score + 0.4 * concentration_score + 0.1 * uses_similarity + 0.1 * side_effects_similarity) * 100\n",
        "\n",
        "    return compatibility_score\n",
        "\n",
        "# واجهة المستخدم\n",
        "def user_test(df, model):\n",
        "    print(\"مرحباً بك في اختبار ملائمة الدواء!\")\n",
        "    composition_clean = input(\"أدخل اسم المادة الفعالة: \")\n",
        "    concentration = input(\"أدخل التركيز: \")\n",
        "    uses = input(\"أدخل الاستخدامات: \")\n",
        "    side_effects = input(\"أدخل الآثار الجانبية: \")\n",
        "    dispensed_medicine = input(\"أدخل اسم الدواء المصروف: \")\n",
        "\n",
        "    prescription = {\n",
        "        'composition_clean': composition_clean,\n",
        "        'concentration': concentration,\n",
        "        'Uses': uses,\n",
        "        'Side_effects': side_effects\n",
        "    }\n",
        "\n",
        "    compatibility_score = calculate_compatibility(prescription, dispensed_medicine, df, model)\n",
        "    print(f'درجة التوافق: {compatibility_score:.2f}%')\n",
        "\n",
        "# إجراء الاختبار\n",
        "user_test(df, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKztwc1Lgy5E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# حفظ النموذج في Google Drive\n",
        "model.save('/content/drive/MyDrive/my_model')\n",
        "\n",
        "print(\"تم حفظ النموذج بنجاح في Google Drive في مجلد 'my_model'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxVjEUAO0rId"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPhu7gD74PNX7oeokLmXhpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}